docker run -d \
  --name ollama-ipex \
  --device /dev/dri \
  --network=host \
  -v ~/ollama-ipex:/root/.ollama \
  -e OLLAMA_NUM_GPU=999 \
  -e OLLAMA_INTEL_GPU=true \
  -e ONEAPI_DEVICE_SELECTOR=level_zero:gpu \
  -e SYCL_CACHE_PERSISTENT=1 \
  -e ZES_ENABLE_SYSMAN=1 \
  --restart unless-stopped \
  intelanalytics/ipex-llm-inference-cpp-xpu:latest \
  bash -c "init-ollama && /usr/local/lib/python3.11/dist-packages/bigdl/cpp/libs/ollama/ollama serve"

모델 pull

# 호스트에서 직접
curl http://localhost:11434/api/pull -d '{"name": "qwen3:8b"}'

# 또는 컨테이너 안에서
docker exec -it ollama-ipex bash

export PATH=$PATH:/usr/local/lib/python3.11/dist-packages/bigdl/cpp/libs/ollama
ollama pull qwen3:8b
ollama run qwen3:8b
